# (AI Math 3강) 경사하강법 - 순한맛
- 미분 : 주어진 점의 **접선의 기울기**
    - 함수값 증가 : 미분값 더하기
    - 함수값 감소 : 미분값 빼기
    ```python3
    import sympy as sym
    from sympy.abc import x

    sym.diff(sym.poly(x ** 2 + x * x + 3), x)
    >>> Poly(2 * x + 2, x, domain = 'zz)
    ```
- 경사상승법, 경사하강법
    - 경사상승법 : 미분값을 더함, 극댓값의 위치 파악
    - 경사하강법 : 미분값을 뺌, 극소값의 위치 파악
    - 극(대, 소)값 도달시 끝

- 경사하강법 : 알고리즘
    ```python3
    def func(val):
        fun = sym.poly(x ** 2 + 2 * x + 3)
        return fun.subs(x, val), fun

    
    def func_gradient(fun, val):
        _, function = fun(val)
        diff = sym.diff(function, x)
        return diff.subs(x, val), diff


    def gradient_descent(funm init_point, lr= 1e-2, ep=1e-5):
        cnt = 0
        val = init_point
        diff, _ = func_gradient(fun, init_point)
        while np.abs(diff) > epsilon:
            val = val - lr * diff
            diff, _ = func_gradient(fun, val)
            cnt += 1
        
        print(f"함수 : {fun(val)[1]}, 연산횟수 : {cnt}, 최소점 : ({val}, {fun(val)[0]})")

    gradient_descent(fun = func, init_point = np.random.
    (-2, 2))

    ```
- Gradient Vector : 벡터 편미분
    - EX. f(x, y) =x ^ 2 + 2 y^2 >>> +/- (2x, 4y)
    - 주어진 함수의 극대 극소값을 찾는데 활용함.
5
- 경사하강법 : 미분이 가능하고 볼록(convex)한 함수에 대해선 적절한 학습률과 학습횟수를 했을 때 수렴이 보장
    - 선형회귀의 경우 L2은 회귀계수 베타에 대해 볼록함수이기 때문에 수렴 보장
    - 비선형회귀 : 볼록성 보장 x → SGD 제안
- 확률적 경사하강법(SGD)
    - 하나(SGD) 또는 일부 데이터(미니배치SGD) 사용 
    - 미니배치를 가지고 그레디언트 벡터 계산(근사)
    - 볼록이 아닌 목적식에서도 사용가능 > 머신러닝 학습에 더 효율적
    - 학습률, 학습횟수, 미니배치 사이즈도 고려해야함
    - 하드웨어적 한계 등으로 인해서 경사하강법은 한계 > SGD 사용
---
# (AI Math 5강) 딥러닝 학습방법 이해하기 
-  softmax
    - 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산
    - 분류문제에서 활용
    - 추론할 때는 원-핫벡터 / 학습할 때는 softmax
- 활성화 함수
    - 비선형함수
    - 활성함수 사용 x 딥러닝 > 선형모형과 다를게 없음.
    - sigmoid, tanh, **relu**
- 신경망    
    - 선형모델 + 활성함수
    - 다층퍼셉트론(MLP) : 신경망이 여러층 합성된 함수
    - 많은 층을 쌓는 이유 : 목적함수를 근사하는데 필요한 노드의 숫자가 훨씬 줄어듦 > 효율적으로 학습 가능
    - 층이 깊다 != 최적화 > 층이 깊어질수록 딥러닝은 어려워질 수 있음.
- 역전파(Backpropagation) :
    - **연쇄법칙**(합성함수) 기반의 자동 미분
---
# (AI Math 6강) 확률론 맛보기
- 필요성
    - 회귀분석 : 예측오차의 분산을 가장 최소화하는 방향으로 학습
    - 분류문제 : 모델 예측의 불확실성을 최소화하는 방향으로 학습
- 이산확률변수(discrete) vs 연속확률변수(continuous)
    - 이산형 확률변수 : **확률변수가 가질 수 있는 경우의 수**를 모두 고려하여 **확률을 더해서** 모델링
    - 연속확률변수 :  **데이터 공간에 정의된 확률변수의 밀도**위에서의 **적분을 통해** 모델링
        - 밀도는 누적확률분포의 변환율을 모델링하며 확률로 해석해서는 안된다.
- 조건부확률과 기계학습
    - 연속확률분포의 경우 P(y|X)는 확률이 아니고 밀도로 해석
- **몬테카를로 샘플링** : 확률분포를 모름 But 샘플링 앎 > 기대값 계산 가능
    - 이산형,연속형 상관없이 성립
    - 독립추출 보장 > 대수의 법칙에 의해서 수렴성 보장
---
